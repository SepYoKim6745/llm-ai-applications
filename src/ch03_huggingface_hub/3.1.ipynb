{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "956c4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.40.1 datasets==2.19.0 huggingface_hub==0.23.0 -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6663b9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 3.1) BERT와 GPT-2 모델을 활용할 때 허깅페이스 트랜스포머 코드 비교\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "text = 'What is Huggingface Transformers?'\n",
    "\n",
    "# BERT 모델 활용\n",
    "bert_model = AutoModel.from_pretrained('bert-base-uncased') # 모델 불러오기\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') # 토큰나이저 불러오기\n",
    "encoded_input = bert_tokenizer(text, return_tensors='pt') # 입력 토큰화\n",
    "bert_output = bert_model(**encoded_input) # 모델에 입력\n",
    "\n",
    "# GPT 모델 활용\n",
    "gpt_model = AutoModel.from_pretrained('gpt2') # 모델 불러오기\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained('gpt2') # 토큰나이저 불러오기\n",
    "encoded_input = gpt_tokenizer(text, return_tensors='pt') # 입력 토큰화\n",
    "bert_output = gpt_model(**encoded_input) # 모델에 입력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "234771f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 예제 3.2) 모델 아이디로 모델 불러오기\n",
    "from transformers import AutoModel\n",
    "\n",
    "model_id = 'klue/roberta-base'\n",
    "model = AutoModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f8e253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 3.4) 분류 헤드가 포함된 모델 불러오기\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model_id = 'SamLowe/roberta-base-go_emotions'\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c9fa195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 예제 3.6) 분류 헤드가 랜덤으로 초기화된 모델 불러오기\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model_id = 'klue/roberta-base'\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2dff02c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 3.8) 토크나이저 불러오기\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "model_id = 'klue/roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1f79cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 1793, 2855, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 18, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', '토', '##큰', '##나이', '##저', '##는', '텍스트', '##를', '토', '##큰', '단위', '##로', '나눈다', '.', '[SEP]']\n",
      "[CLS] 토큰나이저는 텍스트를 토큰 단위로 나눈다. [SEP]\n",
      "토큰나이저는 텍스트를 토큰 단위로 나눈다.\n"
     ]
    }
   ],
   "source": [
    "# 예제 3.9) 토큰나이저 사용하기\n",
    "\n",
    "tokenized = tokenizer('토큰나이저는 텍스트를 토큰 단위로 나눈다.')\n",
    "print(tokenized)\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized['input_ids']))\n",
    "print(tokenizer.decode(tokenized['input_ids']))\n",
    "print(tokenizer.decode(tokenized['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0aefaaaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2], [0, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제 3.10) 토큰나이저에 여러 문장 넣기\n",
    "\n",
    "tokenizer(['첫 번째 문장', '두 번째 문장'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4bb7e25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제 3.11) 하나의 데이터에 여러 문장이 들어가는 경우\n",
    "\n",
    "tokenizer([['첫 번째 문장', '두 번째 문장']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e05e77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] 첫 번째 문장 [SEP]', '[CLS] 두 번째 문장 [SEP]']\n",
      "['[CLS] 첫 번째 문장 [SEP] 두 번째 문장 [SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 예제 3.12) 토큰 아이디를 문자열로 복원\n",
    "\n",
    "first_tokenized_result = tokenizer(['첫 번째 문장', '두 번째 문장'])['input_ids']\n",
    "print(tokenizer.batch_decode(first_tokenized_result))\n",
    "\n",
    "second_tokenized_result = tokenizer([['첫 번째 문장', '두 번째 문장']])['input_ids']\n",
    "print(tokenizer.batch_decode(second_tokenized_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a36387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[2, 1656, 1141, 3135, 6265, 3, 864, 1141, 3135, 6265, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[0, 9502, 3645, 2, 2, 10815, 3645, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# 예제 3.13) BERT 토큰 나이저와 RoBERTa 토큰 나이저\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "print(bert_tokenizer([['첫 번째 문장', '두 번째 문장']]))\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
    "print(roberta_tokenizer([['첫 번째 문장', '두 번째 문장']]))\n",
    "\n",
    "en_roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "print(en_roberta_tokenizer([['first sentence', 'second sentence']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c72813a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2073, 1599, 2062, 18, 2, 1, 1, 1, 1, 1, 1, 1], [0, 864, 1141, 3135, 6265, 2073, 1656, 1141, 3135, 6265, 2178, 2062, 831, 647, 2062, 18, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제 3.14) attention_mask 확인\n",
    "\n",
    "tokenizer(['첫 번째 문장은 짧다.', '두 번째 문장은 첫 번째 문장보다 더 길다.'], padding='longest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f3ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 17554/17554 [00:00<00:00, 153072.21 examples/s]\n",
      "Generating validation split: 100%|██████████| 5841/5841 [00:00<00:00, 169650.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 예제 3.15) KLUE MRC 데이터셋 다운로드\n",
    "from datasets import load_dataset\n",
    "klue_mrc_dataset = load_dataset('klue', 'mrc')\n",
    "# klue_mrc_dataset_only_train = load_dataset('klue', 'mrc', split='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 3.16) 로컬의 데이터 활용하기\n",
    "\n",
    "# 로컬의 csv 데이터 파일을 활용\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv', data_files='my_file.csv')\n",
    "\n",
    "# 파이썬 딕셔너리 활용\n",
    "from datasets import Dataset\n",
    "my_dict = {'a': [1,2,3]}\n",
    "dataset = Dataset.from_dict(my_dict)\n",
    "\n",
    "# 판다스 데이터프레임 활용\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"a\": [1,2,3]})\n",
    "dataset =Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c6192b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-ai-app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
